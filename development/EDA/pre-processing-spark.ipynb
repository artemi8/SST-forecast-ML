{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["!pip install pyspark\n","!pip install xarray"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-12T15:35:06.565141Z","iopub.status.busy":"2024-05-12T15:35:06.564746Z","iopub.status.idle":"2024-05-12T15:35:07.557102Z","shell.execute_reply":"2024-05-12T15:35:07.555867Z","shell.execute_reply.started":"2024-05-12T15:35:06.565108Z"},"trusted":true},"outputs":[],"source":["import xarray as xr\n","import pandas as pd\n","import numpy as np\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, last\n","from pyspark.sql.window import Window\n","from tqdm import tqdm\n","from pyspark.ml.feature import VectorAssembler, RobustScaler\n","from pyspark.ml import Pipeline\n","import sys\n","from pyspark.ml.linalg import Vectors, DenseVector, VectorUDT\n","import pyspark.sql.functions as F\n","from pyspark.sql.functions import udf, struct\n","from pyspark.sql.functions import col as Fcol\n","\n","from pyspark.ml.feature import SQLTransformer\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-12T15:35:07.559451Z","iopub.status.busy":"2024-05-12T15:35:07.558742Z","iopub.status.idle":"2024-05-12T15:35:07.568246Z","shell.execute_reply":"2024-05-12T15:35:07.566678Z","shell.execute_reply.started":"2024-05-12T15:35:07.559410Z"},"trusted":true},"outputs":[],"source":["def get_chunk_as_dataframe(dataset, chunk_index):\n","    \"\"\"\n","    Selects a specified chunk along the 'time' dimension and converts it to a DataFrame.\n","    \"\"\"\n","    time_chunks = dataset.chunks['time']\n","    num_chunks = len(time_chunks)\n","    \n","    if chunk_index >= num_chunks:\n","        raise IndexError(f\"Chunk index {chunk_index} out of bounds for axis 'time' with {num_chunks} chunks.\")\n","    \n","    start_idx = sum(time_chunks[:chunk_index])\n","    end_idx = start_idx + time_chunks[chunk_index]\n","    dataset_chunk = dataset.isel(time=slice(start_idx, end_idx))\n","    print(dataset_chunk)\n","    \n","    return dataset_chunk.to_dataframe().reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["file_path = \"/kaggle/input/era5-82-23-three-hours/adaptor.mars.internal-1714934325.815979-26713-14-d4ecb07d-ed0d-4eca-940f-48bdda8774ae.nc\"\n","dataset = xr.open_dataset(file_path, chunks={'time': 100})\n","chunk_df = get_chunk_as_dataframe(dataset, 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["chunk_df.to_csv(\"spark_chunk.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Start a Spark session\n","spark = SparkSession.builder.master(\"local\").appName(\"NetCDF\").getOrCreate()\n","\n","# Convert pandas DataFrame to Spark DataFrame\n","# df = spark.createDataFrame(chunk_df)\n","chunk_sparkdf=spark.read.csv('spark_chunk.csv', inferSchema=True, header = True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Drop features\n","\n","chunk_sparkdf = chunk_sparkdf.dropna(subset=[\"sst\"])\n","\n","\n","features_to_remove = [\n","    'cdir', 'msdrswrf', 'msdrswrfcs', 'msdwswrf', 'msdwswrfcs', 'msdwuvrf',\n","    'msnswrf', 'msnswrfcs', 'mtdwswrf', 'mtnswrf', 'mtnswrfcs', 'ssr', 'ssrc',\n","    'ssrdc', 'ssrd', 'tsr', 'tsrc', 'fdir'\n","]\n","hand_selected_features = ['mwp', 'pp1d', 'mwd', 'swh', 'expver', 'siconc']\n","\n","chunk_sparkdf = chunk_sparkdf.drop(*features_to_remove, *hand_selected_features)\n","\n","NaN_cols = [col for col in chunk_sparkdf.columns if chunk_sparkdf.filter(chunk_sparkdf[col].isNull()).count() > 0]\n","\n","for col in tqdm(NaN_cols):\n","    chunk_sparkdf = chunk_sparkdf.withColumn(\n","        col,\n","        last(col, ignorenulls=True).over(Window.partitionBy(\"latitude\", \"longitude\").orderBy(\"time\").rowsBetween(0, sys.maxsize))\n","    )\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4945497,"sourceId":8327189,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
